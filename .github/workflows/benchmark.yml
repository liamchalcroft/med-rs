name: Benchmarks

on:
  push:
    branches: [main, master]
  pull_request:
    branches: [main, master]
  schedule:
    # Run weekly on Sunday at 00:00 UTC
    - cron: "0 0 * * 0"
  workflow_dispatch:
    inputs:
      iterations:
        description: "Number of benchmark iterations"
        required: false
        default: "30"

permissions:
  contents: read
  pull-requests: write

env:
  CARGO_TERM_COLOR: always

jobs:
  benchmark:
    name: Benchmark (${{ matrix.os }}, Python ${{ matrix.python }})
    runs-on: ${{ matrix.os }}
    strategy:
      fail-fast: false
      matrix:
        include:
          # Linux x64
          - os: ubuntu-latest
            python: "3.10"
          - os: ubuntu-latest
            python: "3.12"
          # macOS arm64 (Apple Silicon)
          - os: macos-latest
            python: "3.10"
          - os: macos-latest
            python: "3.12"
          # Windows x64
          - os: windows-latest
            python: "3.10"
          - os: windows-latest
            python: "3.12"

    steps:
      - uses: actions/checkout@v4

      - name: Set up Python ${{ matrix.python }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}

      - name: Install Rust
        uses: dtolnay/rust-toolchain@stable

      - name: Cache Rust
        uses: Swatinem/rust-cache@v2

      - name: Install maturin
        run: pip install maturin

      - name: Build medrs wheel
        run: maturin build --release --features python

      - name: Install medrs
        shell: bash
        run: pip install target/wheels/*.whl

      - name: Install dependencies
        run: |
          pip install numpy nibabel
          pip install torch --index-url https://download.pytorch.org/whl/cpu
          pip install monai

      - name: Run medrs benchmarks
        run: python benchmarks/bench_medrs.py --quick

      - name: Run MONAI benchmarks
        run: python benchmarks/bench_monai.py --quick

      - name: Generate comparison
        run: python benchmarks/compare_all.py

      - name: Upload benchmark results
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results-${{ matrix.os }}-py${{ matrix.python }}
          path: benchmarks/results/
          retention-days: 90

  report:
    name: Generate Report
    needs: benchmark
    runs-on: ubuntu-latest
    if: always()
    steps:
      - uses: actions/checkout@v4

      - name: Download all artifacts
        uses: actions/download-artifact@v4
        with:
          path: benchmark-results
          pattern: benchmark-results-*
          merge-multiple: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install dependencies
        run: pip install numpy

      - name: Generate comparison report
        id: report
        run: |
          python << 'EOF'
          import json
          import os
          from pathlib import Path

          results_dir = Path("benchmark-results")
          report_lines = []

          report_lines.append("## Benchmark Results\n")

          # Load medrs and monai results
          medrs_path = results_dir / "medrs_results.json"
          monai_path = results_dir / "monai_results.json"

          if not medrs_path.exists() and not monai_path.exists():
              report_lines.append("No benchmark results found.")
          else:
              medrs_data = json.load(open(medrs_path)) if medrs_path.exists() else None
              monai_data = json.load(open(monai_path)) if monai_path.exists() else None

              report_lines.append("| Operation | Size | medrs (ms) | MONAI (ms) | Speedup |")
              report_lines.append("|-----------|------|------------|------------|---------|")

              if medrs_data and monai_data:
                  # Index monai results by (op, size)
                  monai_by_key = {}
                  for r in monai_data.get("results", []):
                      key = (r["operation"], tuple(r["size"]))
                      monai_by_key[key] = r

                  for result in medrs_data.get("results", []):
                      op = result["operation"]
                      size = tuple(result["size"])
                      medrs_ms = result["median_ms"]

                      key = (op, size)
                      if key in monai_by_key:
                          monai_ms = monai_by_key[key]["median_ms"]
                          speedup = monai_ms / medrs_ms if medrs_ms > 0 else 0
                          size_str = f"{size[0]}Â³"
                          report_lines.append(
                              f"| {op} | {size_str} | {medrs_ms:.2f} | {monai_ms:.2f} | **{speedup:.0f}x** |"
                          )

          report = "\n".join(report_lines)
          print(report)

          # Save report
          with open("benchmark-report.md", "w") as f:
              f.write(report)
          EOF

      - name: Comment on PR
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const report = fs.readFileSync('benchmark-report.md', 'utf8');

            // Find existing comment
            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes('## Benchmark Results')
            );

            const body = `## Benchmark Results\n\n${report}\n\n*Updated: ${new Date().toISOString()}*`;

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: body,
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: body,
              });
            }

      - name: Upload report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-report
          path: benchmark-report.md
          retention-days: 90
